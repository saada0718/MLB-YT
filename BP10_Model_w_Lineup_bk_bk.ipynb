{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7b1806e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/homebrew/lib/python3.11/site-packages (25.0)\n",
      "Requirement already satisfied: lightgbm in /opt/homebrew/lib/python3.11/site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/homebrew/lib/python3.11/site-packages (from lightgbm) (1.25.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (from lightgbm) (1.12.0)\n",
      "Requirement already satisfied: MLB-StatsAPI in /opt/homebrew/lib/python3.11/site-packages (1.8.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from MLB-StatsAPI) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->MLB-StatsAPI) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->MLB-StatsAPI) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->MLB-StatsAPI) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->MLB-StatsAPI) (2024.2.2)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (3.8.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/saadahmed/Library/Python/3.11/lib/python/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/saadahmed/Library/Python/3.11/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saadahmed/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: pyBMA in /opt/homebrew/lib/python3.11/site-packages (0.1.1)\n",
      "Requirement already satisfied: lifelines in /opt/homebrew/lib/python3.11/site-packages (from pyBMA) (0.30.0)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from pyBMA) (2.2.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from pyBMA) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/homebrew/lib/python3.11/site-packages (from lifelines->pyBMA) (1.12.0)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /opt/homebrew/lib/python3.11/site-packages (from lifelines->pyBMA) (3.8.3)\n",
      "Requirement already satisfied: autograd>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from lifelines->pyBMA) (1.7.0)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /opt/homebrew/lib/python3.11/site-packages (from lifelines->pyBMA) (0.5.0)\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /opt/homebrew/lib/python3.11/site-packages (from lifelines->pyBMA) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/saadahmed/Library/Python/3.11/lib/python/site-packages (from pandas->pyBMA) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->pyBMA) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas->pyBMA) (2024.1)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from formulaic>=0.2.2->lifelines->pyBMA) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/saadahmed/Library/Python/3.11/lib/python/site-packages (from formulaic>=0.2.2->lifelines->pyBMA) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from formulaic>=0.2.2->lifelines->pyBMA) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines->pyBMA) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines->pyBMA) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines->pyBMA) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines->pyBMA) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/saadahmed/Library/Python/3.11/lib/python/site-packages (from matplotlib>=3.0->lifelines->pyBMA) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines->pyBMA) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines->pyBMA) (3.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saadahmed/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->pyBMA) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m pip install --upgrade pip\n",
    "!pip install --upgrade lightgbm\n",
    "!pip install MLB-StatsAPI\n",
    "!pip install pyBMA\n",
    "!pip install matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3156859-b802-4717-9acc-304a4cb5acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsapi\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import structureboost as stb\n",
    "import ml_insights as mli\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from structureboost import log_loss\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.max_rows',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3c9670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4n/m6y9jtrx6m1_59wkdvlnxsm80000gn/T/ipykernel_2059/3503072021.py:1: DtypeWarning: Columns (13,14,15,19,85,87,159,198) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('df_bp9_new.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('df_bp9_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56eb3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df['team_h'], prefix='stadium')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934b21ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_win_streak = {\n",
    "\n",
    "}\n",
    "\n",
    "def get_win_streak(home_team, visiting_team, home_team_win, ret_home_win_streak):\n",
    "\n",
    "    prev_home_win_rate = 0\n",
    "    prev_visitor_win_rate = 0\n",
    "\n",
    "    if home_team in team_win_streak:\n",
    "        \n",
    "        prev_home_win_rate = team_win_streak[home_team]\n",
    "\n",
    "    if visiting_team in team_win_streak:\n",
    "        \n",
    "        prev_visitor_win_rate = team_win_streak[visiting_team]\n",
    "\n",
    "    if home_team not in team_win_streak:\n",
    "\n",
    "        if home_team == 1:\n",
    "\n",
    "            team_win_streak[home_team] = 1\n",
    "            team_win_streak[visiting_team] = -1\n",
    "\n",
    "        else:\n",
    "\n",
    "            team_win_streak[home_team] = -1\n",
    "            team_win_streak[visiting_team] = 1\n",
    "\n",
    "    elif visiting_team not in team_win_streak:\n",
    "\n",
    "        if home_team == 1:\n",
    "\n",
    "            team_win_streak[home_team] = 1\n",
    "            team_win_streak[visiting_team] = -1\n",
    "\n",
    "        else:\n",
    "\n",
    "            team_win_streak[home_team] = -1\n",
    "            team_win_streak[visiting_team] = 1\n",
    "\n",
    "    else:\n",
    "        \n",
    "        if home_team_win == 1:\n",
    "\n",
    "            # The home team was on a losing streak and now won the game\n",
    "            if prev_home_win_rate < 0:\n",
    "                team_win_streak[home_team] = 1\n",
    "            # The home team was on a winning streak and won the game\n",
    "            else:\n",
    "                team_win_streak[home_team] += 1\n",
    "\n",
    "            # The visiting team was on a winning streak and now lost the game\n",
    "            if prev_visitor_win_rate > 0:\n",
    "                team_win_streak[visiting_team] = -1\n",
    "            # The visiting team was on a losing streak and lost the game\n",
    "            else:\n",
    "                team_win_streak[visiting_team] += -1\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            # The home team was on a winning streak and now lost the game\n",
    "            if prev_home_win_rate > 0:\n",
    "                team_win_streak[home_team] = -1\n",
    "            # The home team was on a losing streak and lost the game\n",
    "            else:\n",
    "                team_win_streak[home_team] += -1\n",
    "\n",
    "            # The visiting team was on a losing streak and now won the game\n",
    "            if prev_visitor_win_rate < 0:\n",
    "                team_win_streak[visiting_team] = 1\n",
    "            # The visiting team was on a winning streak and won the game\n",
    "            else:\n",
    "                team_win_streak[visiting_team] += 1\n",
    "\n",
    "\n",
    "    if ret_home_win_streak:\n",
    "        return prev_home_win_rate\n",
    "    else:\n",
    "        return prev_visitor_win_rate\n",
    "\n",
    "df['home_win_streak'] = df.apply(lambda row: get_win_streak(row['team_h'], row['team_v'], row['home_victory'], True), axis=1)\n",
    "\n",
    "team_win_streak = {}\n",
    "\n",
    "df['visitor_win_streak'] = df.apply(lambda row: get_win_streak(row['team_h'], row['team_v'], row['home_victory'], False), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3a16be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['date'].apply(lambda x: str(x)[4:6])\n",
    "df['day'] = df['date'].apply(lambda x: str(x)[6:8])\n",
    "df['year'] = df['date'].apply(lambda x: str(x)[0:4])\n",
    "\n",
    "df.month.unique()\n",
    "\n",
    "cumulative_runs = {}\n",
    "game_counts = {}\n",
    "\n",
    "def calculate_avg(team, runs, month, season):\n",
    "\n",
    "    if month <= '06':\n",
    "\n",
    "        if season not in cumulative_runs:\n",
    "\n",
    "            cumulative_runs[season] = { team: runs }\n",
    "            game_counts[season] = { team: 1 }\n",
    "\n",
    "        elif team not in cumulative_runs[season]:\n",
    "\n",
    "            cumulative_runs[season][team] = runs\n",
    "            game_counts[season][team] = 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            cumulative_runs[season][team] += runs\n",
    "            game_counts[season][team] += 1\n",
    "\n",
    "        season = str(int(season) - 1)\n",
    "\n",
    "        if season not in cumulative_runs:\n",
    "            return None\n",
    "\n",
    "        avg = cumulative_runs[season][team] / game_counts[season][team]\n",
    "\n",
    "        return avg\n",
    "\n",
    "    else:\n",
    "\n",
    "        if season not in cumulative_runs:\n",
    "            print(season, month)\n",
    "            sys.exit()\n",
    "\n",
    "        if team not in cumulative_runs[season]:\n",
    "            print(\"ERROR: SOME HOW\")\n",
    "            return None\n",
    "\n",
    "        avg = cumulative_runs[season][team] / game_counts[season][team]\n",
    "\n",
    "        cumulative_runs[season][team] += runs\n",
    "        game_counts[season][team] += 1\n",
    "\n",
    "        return avg\n",
    "\n",
    "\n",
    "df['avg_runs_v'] = df.apply(lambda row: calculate_avg(row['team_v'], row['runs_v'], row['month'], row['season']), axis=1 )\n",
    "df['avg_runs_h'] = df.apply(lambda row: calculate_avg(row['team_h'], row['runs_h'], row['month'], row['season']), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24b45402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cumulative_runs = {}\n",
    "game_counts = {}\n",
    "\n",
    "def calc_runs_allowed(runs_scored_on_team, team, month, season):\n",
    "\n",
    "\n",
    "    if month <= '06':\n",
    "\n",
    "        if season not in cumulative_runs:\n",
    "\n",
    "            cumulative_runs[season] = {\n",
    "                team: runs_scored_on_team\n",
    "            }\n",
    "\n",
    "            game_counts[season] = {\n",
    "                team: 1\n",
    "            }\n",
    "\n",
    "        elif team not in cumulative_runs[season]:\n",
    "\n",
    "            cumulative_runs[season][team] = runs_scored_on_team\n",
    "            game_counts[season][team] = 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            cumulative_runs[season][team] += runs_scored_on_team\n",
    "            game_counts[season][team] += 1\n",
    "\n",
    "        season = str(int(season) - 1)\n",
    "\n",
    "        if season not in cumulative_runs:\n",
    "            return None\n",
    "\n",
    "        avg = cumulative_runs[season][team] / game_counts[season][team]\n",
    "\n",
    "        return avg\n",
    "\n",
    "    else:\n",
    "\n",
    "        if season not in cumulative_runs:\n",
    "            print(season, month)\n",
    "            sys.exit()\n",
    "\n",
    "        if team not in cumulative_runs[season]:\n",
    "            print(\"ERROR: SOME HOW\")\n",
    "            return None\n",
    "\n",
    "        avg = cumulative_runs[season][team] / game_counts[season][team]\n",
    "\n",
    "        cumulative_runs[season][team] += runs_scored_on_team\n",
    "        game_counts[season][team] += 1\n",
    "\n",
    "        return avg\n",
    "\n",
    "df['avg_runs_allowed_v'] = df.apply(lambda row: calc_runs_allowed(row['runs_h'], row['team_v'], row['month'], row['season']), axis=1 )\n",
    "df['avg_runs_allowed_h'] = df.apply(lambda row: calc_runs_allowed(row['runs_v'], row['team_h'], row['month'], row['season']), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "455ca0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_time_zones = {\n",
    "    \"TOR\": \"Eastern\",  # Toronto Blue Jays\n",
    "    \"ATL\": \"Eastern\",  # Atlanta Braves\n",
    "    \"BAL\": \"Eastern\",  # Baltimore Orioles\n",
    "    \"DET\": \"Eastern\",  # Detroit Tigers\n",
    "    \"BOS\": \"Eastern\",  # Boston Red Sox\n",
    "    \"MIN\": \"Central\",  # Minnesota Twins\n",
    "    \"NYA\": \"Eastern\",  # New York Yankees\n",
    "    \"LAN\": \"Pacific\",  # Los Angeles Dodgers\n",
    "    \"CHN\": \"Central\",  # Chicago Cubs\n",
    "    \"SFN\": \"Pacific\",  # San Francisco Giants\n",
    "    \"PIT\": \"Eastern\",  # Pittsburgh Pirates\n",
    "    \"CLE\": \"Eastern\",  # Cleveland Guardians\n",
    "    \"MON\": \"Eastern\",  # Montreal Expos (Defunct, historically Eastern)\n",
    "    \"SEA\": \"Pacific\",  # Seattle Mariners\n",
    "    \"KCA\": \"Central\",  # Kansas City Royals\n",
    "    \"PHI\": \"Eastern\",  # Philadelphia Phillies\n",
    "    \"MIL\": \"Central\",  # Milwaukee Brewers\n",
    "    \"CIN\": \"Eastern\",  # Cincinnati Reds\n",
    "    \"NYN\": \"Eastern\",  # New York Mets\n",
    "    \"HOU\": \"Central\",  # Houston Astros (historically Central, now in American League)\n",
    "    \"SLN\": \"Central\",  # St. Louis Cardinals\n",
    "    \"SDN\": \"Pacific\",  # San Diego Padres\n",
    "    \"CHA\": \"Central\",  # Chicago White Sox\n",
    "    \"TEX\": \"Central\",  # Texas Rangers\n",
    "    \"CAL\": \"Pacific\",  # California Angels (Now Los Angeles Angels)\n",
    "    \"OAK\": \"Pacific\",  # Oakland Athletics\n",
    "    \"COL\": \"Mountain\", # Colorado Rockies\n",
    "    \"FLO\": \"Eastern\",  # Florida Marlins (Now Miami Marlins)\n",
    "    \"ANA\": \"Pacific\",  # Anaheim Angels (Now Los Angeles Angels)\n",
    "    \"TBA\": \"Eastern\",  # Tampa Bay Rays\n",
    "    \"ARI\": \"Mountain\", # Arizona Diamondbacks\n",
    "    \"WAS\": \"Eastern\",  # Washington Nationals\n",
    "    \"MIA\": \"Eastern\",  # Miami Marlins\n",
    "}\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "def hour_difference(timezone1, timezone2):\n",
    "    \"\"\"\n",
    "    Returns the hour difference between two time zones.\n",
    "\n",
    "    Parameters:\n",
    "    timezone1 (str): The name of the first time zone (e.g., 'Eastern', 'Pacific').\n",
    "    timezone2 (str): The name of the second time zone (e.g., 'Central', 'Mountain').\n",
    "\n",
    "    Returns:\n",
    "    int: The hour difference between the two time zones.\n",
    "    \"\"\"\n",
    "    # Mapping shorthand names to actual time zone names\n",
    "    timezone_map = {\n",
    "        \"Eastern\": \"US/Eastern\",\n",
    "        \"Central\": \"US/Central\",\n",
    "        \"Mountain\": \"US/Mountain\",\n",
    "        \"Pacific\": \"US/Pacific\",\n",
    "    }\n",
    "    \n",
    "    # Get the actual time zones\n",
    "    tz1 = pytz.timezone(timezone_map.get(timezone1))\n",
    "    tz2 = pytz.timezone(timezone_map.get(timezone2))\n",
    "    \n",
    "    # Get the current time in UTC\n",
    "    now = datetime.utcnow()\n",
    "    \n",
    "    # Localize the current time to each time zone\n",
    "    offset1 = tz1.utcoffset(now).total_seconds() / 3600\n",
    "    offset2 = tz2.utcoffset(now).total_seconds() / 3600\n",
    "    \n",
    "    # Calculate and return the hour difference\n",
    "    return abs(int(offset1 - offset2))\n",
    "\n",
    "\n",
    "df['hour_diff'] = df.apply(lambda x: hour_difference( team_time_zones[x.team_h], team_time_zones[x.team_v]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77b6c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract year from the date column (already done in your code)\n",
    "df['year'] = df['date'].astype('str')\n",
    "df['year'] = df['year'].apply(lambda x: int(x[:4]))\n",
    "\n",
    "# Step 2: Calculate home win rate grouped by team and year\n",
    "home_win_rate = df.groupby(['team_h', 'year']).agg({'home_victory': 'mean'}).reset_index()\n",
    "home_win_rate['home_win_rate'] = home_win_rate['home_victory']\n",
    "home_win_rate.drop(columns=['home_victory'], inplace=True)\n",
    "\n",
    "# Step 3: Shift the home win rate by year to get the previous year's data\n",
    "home_win_rate['prev_year_home_win_rate'] = home_win_rate.groupby('team_h')['home_win_rate'].shift(1)\n",
    "\n",
    "# Step 4: Merge the previous year's win rate back to the original DataFrame\n",
    "df = pd.merge(df, home_win_rate[['team_h', 'year', 'prev_year_home_win_rate']],\n",
    "              on=['team_h', 'year'],\n",
    "              how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c14b80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_win_rate = df.groupby(['team_v', 'year']).agg({'home_victory': 'mean'}).reset_index()\n",
    "visitor_win_rate['visitor_win_rate'] = visitor_win_rate['home_victory']\n",
    "visitor_win_rate.drop(columns=['home_victory'], inplace=True)\n",
    "\n",
    "# Step 3: Shift the home win rate by year to get the previous year's data\n",
    "visitor_win_rate['prev_year_visitor_win_rate'] = visitor_win_rate.groupby('team_v')['visitor_win_rate'].shift(1)\n",
    "\n",
    "# Step 4: Merge the previous year's win rate back to the original DataFrame\n",
    "df = pd.merge(df, visitor_win_rate[['team_v', 'year', 'prev_year_visitor_win_rate']],\n",
    "              on=['team_v', 'year'],\n",
    "              how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a723daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df.prev_year_home_win_rate.isnull()) | ~(df.prev_year_visitor_win_rate.isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a37991-6f16-4728-af33-09aa7bb10fc3",
   "metadata": {},
   "source": [
    "## Begin Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6449d10-5eab-44e0-807e-80612fd3c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.run_diff!=0]\n",
    "df_train = df[(df.season>1980) & (df.season<=2016) & ~(df.OBP_162_h.isnull())]\n",
    "# df_train = df[(df.season>2000) & (df.season<=2016) & ~(df.OBP_162_h.isnull())]\n",
    "df_valid = df[(df.season>=2017) & (df.season<=2018)]\n",
    "df_test = df[df.season>=2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c22876-12ea-4ea8-a9d4-cb1e59e9c8d4",
   "metadata": {},
   "source": [
    "## Let's add in some lineup features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aaca1af-5ac8-489b-9714-dc02774eac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'home_victory'\n",
    "\n",
    "y_train = df_train[target].to_numpy()\n",
    "y_valid = df_valid[target].to_numpy()\n",
    "y_test = df_test[target].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4089275",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Blending:\n",
    "    def __init__(self, base_model_one, base_model_two, meta_model):\n",
    "        self.base_model_one = base_model_one\n",
    "        self.base_model_two = base_model_two\n",
    "        self.meta_model = meta_model\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "\n",
    "        self.base_model_one.fit(X_train, y_train)\n",
    "        self.base_model_two.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        pred_1 = self.base_model_one.predict_proba(X_val)[:, 1]\n",
    "        pred_2 = self.base_model_two.predict_proba(X_val)[:, 1]\n",
    "\n",
    "\n",
    "        blending_X = np.column_stack((pred_1, pred_2))\n",
    "        blending_y = y_val\n",
    "\n",
    "        self.meta_model.fit(blending_X, blending_y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        test_pred_1 = self.base_model_one.predict_proba(X_test)[:, 1]\n",
    "        test_pred_2 = self.base_model_two.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Create test features for the meta-learner\n",
    "        test_blending_X = np.column_stack((test_pred_1, test_pred_2))\n",
    "\n",
    "        # Use the meta-learner to make the final predictions\n",
    "        final_pred = self.meta_model.predict(test_blending_X)\n",
    "        return final_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a1121d9-1c55-4b1d-8b4b-26ae9268f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res(pred, implied_prob):\n",
    "\n",
    "      if abs(pred - implied_prob) >= 0.1:\n",
    "            return implied_prob\n",
    "      return pred\n",
    "\n",
    "def try_features(feat_set, max_depth=2):\n",
    "    target = 'home_victory'\n",
    "    X_train = df_train.loc[:,feat_set]\n",
    "    X_valid = df_valid.loc[:,feat_set]\n",
    "    X_test = df_test.loc[:,feat_set]\n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy='mean')  # Or 'median', 'most_frequent', etc.\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_valid = imputer.transform(X_valid)\n",
    "    X_test = imputer.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    lgbm1 = lgbm.LGBMClassifier(n_estimators=1000, learning_rate=.02, max_depth=max_depth)\n",
    "    lgbm1.fit(X_train, y_train, eval_set=(X_valid, y_valid), eval_metric='logloss', \n",
    "          callbacks=[lgbm.early_stopping(stopping_rounds=50)])\n",
    "\n",
    "\n",
    "    preds_lgbm_test = lgbm1.predict_proba(X_test)[:,1]\n",
    "    ll_test = log_loss(y_test, preds_lgbm_test)\n",
    "    result = pd.DataFrame(preds_lgbm_test, columns=['result'])\n",
    "\n",
    "    dec_tree = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    log_reg = LogisticRegression(max_iter=1000)\n",
    "    random_forest = RandomForestClassifier(max_depth=max_depth)\n",
    "\n",
    "    stacking_clf = StackingClassifier(estimators=[\n",
    "                                                    # ('xgb', XGBClassifier(n_estimators=1000, learning_rate=.02, max_depth=max_depth)),\n",
    "                                                    # ('dec_tree', dec_tree),\n",
    "                                                    ('random_forest', random_forest),\n",
    "                                                    ('log_reg', log_reg)\n",
    "                                                    ],\n",
    "                                        final_estimator=LogisticRegression(max_iter=1000))\n",
    "\n",
    "\n",
    "    blending_model = Blending(lgbm1, stacking_clf, log_reg)\n",
    "\n",
    "    blending_model.fit(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "    # calibrated_stacking = CalibratedClassifierCV(blending_model, method='sigmoid', cv=\"prefit\")\n",
    "    # calibrated_stacking.fit(X_valid, y_valid)\n",
    "\n",
    "    stacking_preds = blending_model.predict(X_test)\n",
    "\n",
    "    new_model = pd.DataFrame(stacking_preds, columns=['result'])\n",
    "\n",
    "    implied_prob = pd.DataFrame()\n",
    "    implied_prob['implied_prob_h_mid'] = df_test['implied_prob_h_mid']\n",
    "\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['our_result'] = new_model['result']\n",
    "    new_df['implied_prob']  = implied_prob['implied_prob_h_mid']\n",
    "\n",
    "#     new_df['final_result'] = new_df.apply(lambda row: res(row.our_result, row.implied_prob), axis=1)\n",
    "\n",
    "    implied_prob['implied_prob_h_mid'] = (implied_prob['implied_prob_h_mid'] > .5).astype(int)\n",
    "    result['result'] = (result['result'] > 0.5).astype(int)\n",
    "    new_model['result'] = (new_model['result'] > 0.5).astype(int)\n",
    "\n",
    "    test_df = pd.DataFrame()\n",
    "    test_df['result'] = new_model['result']\n",
    "    test_df['implied_prob_h_mid'] = implied_prob['implied_prob_h_mid']\n",
    "\n",
    "#     our_accuracy_rate = round((accuracy_score(y_test, new_model['result']) * 100),2)\n",
    "    our_accuracy_rate = round((accuracy_score(y_test, new_df['our_result']) * 100),2)\n",
    "    yt_accuracy_rate = round((accuracy_score(y_test, result['result']) * 100), 2)\n",
    "\n",
    "#     difference_in_accuracy = round((accuracy_score(y_test, new_model['result']) * 100) - (accuracy_score(y_test, implied_prob['implied_prob_h_mid']) * 100), 2)\n",
    "    difference_in_accuracy = round((accuracy_score(y_test, new_df['our_result']) * 100) - (accuracy_score(y_test, implied_prob['implied_prob_h_mid']) * 100), 2)\n",
    "    difference_in_accuracy_yt = round((accuracy_score(y_test, result['result']) * 100) - (accuracy_score(y_test, implied_prob['implied_prob_h_mid']) * 100), 2)\n",
    "    vegas_accuracy_rate = round((accuracy_score(y_test, implied_prob['implied_prob_h_mid']) *100),2)\n",
    "\n",
    "    print(\"YT model's accuracy score: \", yt_accuracy_rate)\n",
    "    print(\"Vegas model's accuracy score: \", vegas_accuracy_rate)\n",
    "    print(\"Our model's accuracy score: \", our_accuracy_rate)\n",
    "    print(\"Difference in accuracy (YT): \", difference_in_accuracy_yt)\n",
    "    print(\"Difference in accuracy (Our):\", difference_in_accuracy)\n",
    "\n",
    "    return yt_accuracy_rate, vegas_accuracy_rate, our_accuracy_rate, difference_in_accuracy_yt, difference_in_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a89fcdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = [\n",
    "        \"Strt_WHIP_35_v\",\n",
    "        \"Strt_TB_BB_perc_35_h\",\n",
    "        \"Strt_TB_BB_perc_35_v\",\n",
    "        \"Strt_H_BB_perc_35_h\",\n",
    "        \"Strt_H_BB_perc_35_v\",\n",
    "        \"Strt_SO_perc_10_h\",\n",
    "        \"Strt_SO_perc_10_v\",\n",
    "        \"Bpen_WHIP_75_h\",\n",
    "        \"Bpen_WHIP_75_v\",\n",
    "        \"Bpen_TB_BB_perc_75_h\",\n",
    "        \"Bpen_TB_BB_perc_75_v\",\n",
    "        \"Bpen_H_BB_perc_75_h\",\n",
    "        \"Bpen_H_BB_perc_75_v\",\n",
    "        \"Bpen_SO_perc_75_h\",\n",
    "        \"Bpen_SO_perc_75_v\",\n",
    "        \"Bpen_WHIP_35_h\",\n",
    "        \"Bpen_WHIP_35_v\",\n",
    "        \"Bpen_TB_BB_perc_35_h\",\n",
    "        \"Bpen_TB_BB_perc_35_v\",\n",
    "        \"Bpen_H_BB_perc_35_h\",\n",
    "        \"Bpen_H_BB_perc_35_v\",\n",
    "        \"prev_year_home_win_rate\",\n",
    "        \"avg_runs_allowed_h\",\n",
    "        \"avg_runs_allowed_v\",\n",
    "        \"avg_runs_h\",\n",
    "        \"avg_runs_v\",\n",
    "        \"prev_year_visitor_win_rate\",\n",
    "        \"days_between_games_h\",\n",
    "        # \"stadium_ANA\",\n",
    "        # \"stadium_ARI\",\n",
    "        # \"stadium_ATL\",\n",
    "        # \"stadium_BAL\",\n",
    "        # \"stadium_BOS\",\n",
    "        # \"stadium_CAL\",\n",
    "        # \"stadium_CHA\",\n",
    "        # \"stadium_CHN\",\n",
    "        # \"stadium_CIN\",\n",
    "        # \"stadium_CLE\",\n",
    "        # \"stadium_COL\",\n",
    "        # \"stadium_DET\",\n",
    "        # \"stadium_FLO\",\n",
    "        # \"stadium_HOU\",\n",
    "        # \"stadium_KCA\",\n",
    "        # \"stadium_LAN\",\n",
    "        # \"stadium_MIA\",\n",
    "        # \"stadium_MIL\",\n",
    "        # \"stadium_MIN\",\n",
    "        # \"stadium_MON\",\n",
    "        # \"stadium_NYA\",\n",
    "        # \"stadium_NYN\",\n",
    "        # \"stadium_OAK\",\n",
    "        # \"stadium_PHI\",\n",
    "        # \"stadium_PIT\",\n",
    "        # \"stadium_SDN\",\n",
    "        # \"stadium_SEA\",\n",
    "        # \"stadium_SFN\",\n",
    "        # \"stadium_SLN\",\n",
    "        # \"stadium_TBA\",\n",
    "        # \"stadium_TEX\",\n",
    "        # \"stadium_TOR\",\n",
    "        # \"stadium_WAS\",\n",
    "        \"days_between_games_v\",\n",
    "        \"hour_diff\",\n",
    "        \"implied_prob_h\",\n",
    "        \"implied_prob_v\",\n",
    "        \"implied_prob_h_mid\",\n",
    "        \"Bpen_SO_perc_35_h\",\n",
    "        \"Bpen_SO_perc_35_v\",\n",
    "        \"Bpen_WHIP_10_h\",\n",
    "        \"Bpen_WHIP_10_v\",\n",
    "        \"Bpen_TB_BB_perc_10_h\",\n",
    "        \"Bpen_TB_BB_perc_10_v\",\n",
    "        \"Bpen_H_BB_perc_10_h\",\n",
    "        \"Bpen_H_BB_perc_10_v\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d553c-5777-45f8-b494-89e42770e597",
   "metadata": {
    "tags": []
   },
   "source": [
    "### First, let's revisit our best model from our last modeling session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_accuracy_rate, vegas_accuracy_rate, our_accuracy_rate, difference_in_accuracy_yt, difference_in_accuracy = try_features(best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36af9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_model_averaging(predictions, model_weights=None):\n",
    "    \"\"\"\n",
    "    Perform Bayesian Model Averaging.\n",
    "    \n",
    "    predictions: numpy array of shape (num_samples, num_models), each column is a model's probability output.\n",
    "    model_weights: Optional list of weights for models. If None, all models are weighted equally.\n",
    "    \n",
    "    Returns: Averaged probabilities.\n",
    "    \"\"\"\n",
    "    if model_weights is None:\n",
    "        model_weights = np.ones(predictions.shape[1]) / predictions.shape[1]  # Equal weight if not provided\n",
    "\n",
    "    # Normalize weights to sum to 1\n",
    "    model_weights = np.array(model_weights) / np.sum(model_weights)\n",
    "\n",
    "    # Compute weighted average of predictions\n",
    "    return np.sum(predictions * model_weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d645d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')  # Or 'median', 'most_frequent', etc.\n",
    "df[best_features] = imputer.fit_transform(df[best_features])  # Ensure that you're only imputing relevant columns\n",
    "\n",
    "df_test = df[df.season>=2021].loc[:, best_features + ['home_victory']]\n",
    "df_train = df[(df.season>1980) & (df.season<=2016)].loc[:, best_features + ['home_victory']]\n",
    "df_valid = df[(df.season>=2017) & (df.season<=2018)].loc[:, best_features + ['home_victory']]\n",
    "\n",
    "X_train = df_train.drop(columns=['home_victory'])\n",
    "X_valid = df_valid.drop(columns=['home_victory'])\n",
    "X_test = df_test.drop(columns=['home_victory'])\n",
    "\n",
    "y_train = df_train['home_victory']\n",
    "y_valid = df_valid['home_victory']\n",
    "y_test = df_test['home_victory']\n",
    "\n",
    "# Train individual models\n",
    "model_1 = LogisticRegression()\n",
    "model_2 = RandomForestClassifier(n_estimators=100)\n",
    "model_3 = SVC(probability=True)  # Enable probability output for BMA\n",
    "\n",
    "# Fit models on training data\n",
    "model_1.fit(X_train, y_train)\n",
    "model_2.fit(X_train, y_train)\n",
    "model_3.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Get model predictions (probabilities for class 1)\n",
    "preds_1 = model_1.predict_proba(X_test)[:, 1]\n",
    "preds_2 = model_2.predict_proba(X_test)[:, 1]\n",
    "preds_3 = model_3.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Stack predictions\n",
    "predictions = np.column_stack((preds_1, preds_2, preds_3))\n",
    "\n",
    "# Perform Bayesian Model Averaging\n",
    "bma_preds = bayesian_model_averaging(predictions, y_test)\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "final_preds = (bma_preds >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, final_preds)\n",
    "print(f\"BMA Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
