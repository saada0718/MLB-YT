{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.11 -m pip install --upgrade pip\n",
    "!pip install --upgrade lightgbm\n",
    "!pip install MLB-StatsAPI\n",
    "!pip install tensorflow scikit-learn\n",
    "!pip install matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3156859-b802-4717-9acc-304a4cb5acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsapi\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import structureboost as stb\n",
    "import ml_insights as mli\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from structureboost import log_loss\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.max_rows',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_bp9_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df['team_h'], prefix='stadium')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b21ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_win_streak = {\n",
    "\n",
    "}\n",
    "\n",
    "def get_win_streak(home_team, visiting_team, home_team_win, ret_home_win_streak):\n",
    "\n",
    "    prev_home_win_rate = 0\n",
    "    prev_visitor_win_rate = 0\n",
    "\n",
    "    if home_team in team_win_streak:\n",
    "        \n",
    "        prev_home_win_rate = team_win_streak[home_team]\n",
    "\n",
    "    if visiting_team in team_win_streak:\n",
    "        \n",
    "        prev_visitor_win_rate = team_win_streak[visiting_team]\n",
    "\n",
    "    if home_team not in team_win_streak:\n",
    "\n",
    "        if home_team == 1:\n",
    "\n",
    "            team_win_streak[home_team] = 1\n",
    "            team_win_streak[visiting_team] = -1\n",
    "\n",
    "        else:\n",
    "\n",
    "            team_win_streak[home_team] = -1\n",
    "            team_win_streak[visiting_team] = 1\n",
    "\n",
    "    elif visiting_team not in team_win_streak:\n",
    "\n",
    "        if home_team == 1:\n",
    "\n",
    "            team_win_streak[home_team] = 1\n",
    "            team_win_streak[visiting_team] = -1\n",
    "\n",
    "        else:\n",
    "\n",
    "            team_win_streak[home_team] = -1\n",
    "            team_win_streak[visiting_team] = 1\n",
    "\n",
    "    else:\n",
    "        \n",
    "        if home_team_win == 1:\n",
    "\n",
    "            # The home team was on a losing streak and now won the game\n",
    "            if prev_home_win_rate < 0:\n",
    "                team_win_streak[home_team] = 1\n",
    "            # The home team was on a winning streak and won the game\n",
    "            else:\n",
    "                team_win_streak[home_team] += 1\n",
    "\n",
    "            # The visiting team was on a winning streak and now lost the game\n",
    "            if prev_visitor_win_rate > 0:\n",
    "                team_win_streak[visiting_team] = -1\n",
    "            # The visiting team was on a losing streak and lost the game\n",
    "            else:\n",
    "                team_win_streak[visiting_team] += -1\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            # The home team was on a winning streak and now lost the game\n",
    "            if prev_home_win_rate > 0:\n",
    "                team_win_streak[home_team] = -1\n",
    "            # The home team was on a losing streak and lost the game\n",
    "            else:\n",
    "                team_win_streak[home_team] += -1\n",
    "\n",
    "            # The visiting team was on a losing streak and now won the game\n",
    "            if prev_visitor_win_rate < 0:\n",
    "                team_win_streak[visiting_team] = 1\n",
    "            # The visiting team was on a winning streak and won the game\n",
    "            else:\n",
    "                team_win_streak[visiting_team] += 1\n",
    "\n",
    "\n",
    "    if ret_home_win_streak:\n",
    "        return prev_home_win_rate\n",
    "    else:\n",
    "        return prev_visitor_win_rate\n",
    "\n",
    "df['home_win_streak'] = df.apply(lambda row: get_win_streak(row['team_h'], row['team_v'], row['home_victory'], True), axis=1)\n",
    "\n",
    "team_win_streak = {}\n",
    "\n",
    "df['visitor_win_streak'] = df.apply(lambda row: get_win_streak(row['team_h'], row['team_v'], row['home_victory'], False), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a16be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['date'].apply(lambda x: str(x)[4:6])\n",
    "df['day'] = df['date'].apply(lambda x: str(x)[6:8])\n",
    "df['year'] = df['date'].apply(lambda x: str(x)[0:4])\n",
    "\n",
    "df.month.unique()\n",
    "\n",
    "cumulative_runs = {}\n",
    "game_counts = {}\n",
    "\n",
    "def calculate_avg(team, runs, month, season):\n",
    "\n",
    "    if month <= '06':\n",
    "\n",
    "        if season not in cumulative_runs:\n",
    "\n",
    "            cumulative_runs[season] = { team: runs }\n",
    "            game_counts[season] = { team: 1 }\n",
    "\n",
    "        elif team not in cumulative_runs[season]:\n",
    "\n",
    "            cumulative_runs[season][team] = runs\n",
    "            game_counts[season][team] = 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            cumulative_runs[season][team] += runs\n",
    "            game_counts[season][team] += 1\n",
    "\n",
    "        season = str(int(season) - 1)\n",
    "\n",
    "        if season not in cumulative_runs:\n",
    "            return None\n",
    "\n",
    "        avg = cumulative_runs[season][team] / game_counts[season][team]\n",
    "\n",
    "        return avg\n",
    "\n",
    "    else:\n",
    "\n",
    "        if season not in cumulative_runs:\n",
    "            print(season, month)\n",
    "            sys.exit()\n",
    "\n",
    "        if team not in cumulative_runs[season]:\n",
    "            print(\"ERROR: SOME HOW\")\n",
    "            return None\n",
    "\n",
    "        avg = cumulative_runs[season][team] / game_counts[season][team]\n",
    "\n",
    "        cumulative_runs[season][team] += runs\n",
    "        game_counts[season][team] += 1\n",
    "\n",
    "        return avg\n",
    "\n",
    "\n",
    "df['avg_runs_v'] = df.apply(lambda row: calculate_avg(row['team_v'], row['runs_v'], row['month'], row['season']), axis=1 )\n",
    "df['avg_runs_h'] = df.apply(lambda row: calculate_avg(row['team_h'], row['runs_h'], row['month'], row['season']), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b45402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cumulative_runs = {}\n",
    "game_counts = {}\n",
    "\n",
    "def calc_runs_allowed(runs_scored_on_team, team, month, season):\n",
    "\n",
    "\n",
    "    if month <= '06':\n",
    "\n",
    "        if season not in cumulative_runs:\n",
    "\n",
    "            cumulative_runs[season] = {\n",
    "                team: runs_scored_on_team\n",
    "            }\n",
    "\n",
    "            game_counts[season] = {\n",
    "                team: 1\n",
    "            }\n",
    "\n",
    "        elif team not in cumulative_runs[season]:\n",
    "\n",
    "            cumulative_runs[season][team] = runs_scored_on_team\n",
    "            game_counts[season][team] = 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            cumulative_runs[season][team] += runs_scored_on_team\n",
    "            game_counts[season][team] += 1\n",
    "\n",
    "        season = str(int(season) - 1)\n",
    "\n",
    "        if season not in cumulative_runs:\n",
    "            return None\n",
    "\n",
    "        avg = cumulative_runs[season][team] / game_counts[season][team]\n",
    "\n",
    "        return avg\n",
    "\n",
    "    else:\n",
    "\n",
    "        if season not in cumulative_runs:\n",
    "            print(season, month)\n",
    "            sys.exit()\n",
    "\n",
    "        if team not in cumulative_runs[season]:\n",
    "            print(\"ERROR: SOME HOW\")\n",
    "            return None\n",
    "\n",
    "        avg = cumulative_runs[season][team] / game_counts[season][team]\n",
    "\n",
    "        cumulative_runs[season][team] += runs_scored_on_team\n",
    "        game_counts[season][team] += 1\n",
    "\n",
    "        return avg\n",
    "\n",
    "df['avg_runs_allowed_v'] = df.apply(lambda row: calc_runs_allowed(row['runs_h'], row['team_v'], row['month'], row['season']), axis=1 )\n",
    "df['avg_runs_allowed_h'] = df.apply(lambda row: calc_runs_allowed(row['runs_v'], row['team_h'], row['month'], row['season']), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ca0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_time_zones = {\n",
    "    \"TOR\": \"Eastern\",  # Toronto Blue Jays\n",
    "    \"ATL\": \"Eastern\",  # Atlanta Braves\n",
    "    \"BAL\": \"Eastern\",  # Baltimore Orioles\n",
    "    \"DET\": \"Eastern\",  # Detroit Tigers\n",
    "    \"BOS\": \"Eastern\",  # Boston Red Sox\n",
    "    \"MIN\": \"Central\",  # Minnesota Twins\n",
    "    \"NYA\": \"Eastern\",  # New York Yankees\n",
    "    \"LAN\": \"Pacific\",  # Los Angeles Dodgers\n",
    "    \"CHN\": \"Central\",  # Chicago Cubs\n",
    "    \"SFN\": \"Pacific\",  # San Francisco Giants\n",
    "    \"PIT\": \"Eastern\",  # Pittsburgh Pirates\n",
    "    \"CLE\": \"Eastern\",  # Cleveland Guardians\n",
    "    \"MON\": \"Eastern\",  # Montreal Expos (Defunct, historically Eastern)\n",
    "    \"SEA\": \"Pacific\",  # Seattle Mariners\n",
    "    \"KCA\": \"Central\",  # Kansas City Royals\n",
    "    \"PHI\": \"Eastern\",  # Philadelphia Phillies\n",
    "    \"MIL\": \"Central\",  # Milwaukee Brewers\n",
    "    \"CIN\": \"Eastern\",  # Cincinnati Reds\n",
    "    \"NYN\": \"Eastern\",  # New York Mets\n",
    "    \"HOU\": \"Central\",  # Houston Astros (historically Central, now in American League)\n",
    "    \"SLN\": \"Central\",  # St. Louis Cardinals\n",
    "    \"SDN\": \"Pacific\",  # San Diego Padres\n",
    "    \"CHA\": \"Central\",  # Chicago White Sox\n",
    "    \"TEX\": \"Central\",  # Texas Rangers\n",
    "    \"CAL\": \"Pacific\",  # California Angels (Now Los Angeles Angels)\n",
    "    \"OAK\": \"Pacific\",  # Oakland Athletics\n",
    "    \"COL\": \"Mountain\", # Colorado Rockies\n",
    "    \"FLO\": \"Eastern\",  # Florida Marlins (Now Miami Marlins)\n",
    "    \"ANA\": \"Pacific\",  # Anaheim Angels (Now Los Angeles Angels)\n",
    "    \"TBA\": \"Eastern\",  # Tampa Bay Rays\n",
    "    \"ARI\": \"Mountain\", # Arizona Diamondbacks\n",
    "    \"WAS\": \"Eastern\",  # Washington Nationals\n",
    "    \"MIA\": \"Eastern\",  # Miami Marlins\n",
    "}\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "def hour_difference(timezone1, timezone2):\n",
    "    \"\"\"\n",
    "    Returns the hour difference between two time zones.\n",
    "\n",
    "    Parameters:\n",
    "    timezone1 (str): The name of the first time zone (e.g., 'Eastern', 'Pacific').\n",
    "    timezone2 (str): The name of the second time zone (e.g., 'Central', 'Mountain').\n",
    "\n",
    "    Returns:\n",
    "    int: The hour difference between the two time zones.\n",
    "    \"\"\"\n",
    "    # Mapping shorthand names to actual time zone names\n",
    "    timezone_map = {\n",
    "        \"Eastern\": \"US/Eastern\",\n",
    "        \"Central\": \"US/Central\",\n",
    "        \"Mountain\": \"US/Mountain\",\n",
    "        \"Pacific\": \"US/Pacific\",\n",
    "    }\n",
    "    \n",
    "    # Get the actual time zones\n",
    "    tz1 = pytz.timezone(timezone_map.get(timezone1))\n",
    "    tz2 = pytz.timezone(timezone_map.get(timezone2))\n",
    "    \n",
    "    # Get the current time in UTC\n",
    "    now = datetime.utcnow()\n",
    "    \n",
    "    # Localize the current time to each time zone\n",
    "    offset1 = tz1.utcoffset(now).total_seconds() / 3600\n",
    "    offset2 = tz2.utcoffset(now).total_seconds() / 3600\n",
    "    \n",
    "    # Calculate and return the hour difference\n",
    "    return abs(int(offset1 - offset2))\n",
    "\n",
    "\n",
    "df['hour_diff'] = df.apply(lambda x: hour_difference( team_time_zones[x.team_h], team_time_zones[x.team_v]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract year from the date column (already done in your code)\n",
    "df['year'] = df['date'].astype('str')\n",
    "df['year'] = df['year'].apply(lambda x: int(x[:4]))\n",
    "\n",
    "# Step 2: Calculate home win rate grouped by team and year\n",
    "home_win_rate = df.groupby(['team_h', 'year']).agg({'home_victory': 'mean'}).reset_index()\n",
    "home_win_rate['home_win_rate'] = home_win_rate['home_victory']\n",
    "home_win_rate.drop(columns=['home_victory'], inplace=True)\n",
    "\n",
    "# Step 3: Shift the home win rate by year to get the previous year's data\n",
    "home_win_rate['prev_year_home_win_rate'] = home_win_rate.groupby('team_h')['home_win_rate'].shift(1)\n",
    "\n",
    "# Step 4: Merge the previous year's win rate back to the original DataFrame\n",
    "df = pd.merge(df, home_win_rate[['team_h', 'year', 'prev_year_home_win_rate']],\n",
    "              on=['team_h', 'year'],\n",
    "              how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visitor_win_rate = df.groupby(['team_v', 'year']).agg({'home_victory': 'mean'}).reset_index()\n",
    "visitor_win_rate['visitor_win_rate'] = visitor_win_rate['home_victory']\n",
    "visitor_win_rate.drop(columns=['home_victory'], inplace=True)\n",
    "\n",
    "# Step 3: Shift the home win rate by year to get the previous year's data\n",
    "visitor_win_rate['prev_year_visitor_win_rate'] = visitor_win_rate.groupby('team_v')['visitor_win_rate'].shift(1)\n",
    "\n",
    "# Step 4: Merge the previous year's win rate back to the original DataFrame\n",
    "df = pd.merge(df, visitor_win_rate[['team_v', 'year', 'prev_year_visitor_win_rate']],\n",
    "              on=['team_v', 'year'],\n",
    "              how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df.prev_year_home_win_rate.isnull()) | ~(df.prev_year_visitor_win_rate.isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a37991-6f16-4728-af33-09aa7bb10fc3",
   "metadata": {},
   "source": [
    "## Begin Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6449d10-5eab-44e0-807e-80612fd3c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.run_diff!=0]\n",
    "df_train = df[(df.season>1980) & (df.season<=2016) & ~(df.OBP_162_h.isnull())]\n",
    "# df_train = df[(df.season>2000) & (df.season<=2016) & ~(df.OBP_162_h.isnull())]\n",
    "df_valid = df[(df.season>=2017) & (df.season<=2018)]\n",
    "df_test = df[df.season>=2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c22876-12ea-4ea8-a9d4-cb1e59e9c8d4",
   "metadata": {},
   "source": [
    "## Let's add in some lineup features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaca1af-5ac8-489b-9714-dc02774eac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'home_victory'\n",
    "\n",
    "y_train = df_train[target].to_numpy()\n",
    "y_valid = df_valid[target].to_numpy()\n",
    "y_test = df_test[target].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4089275",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Blending:\n",
    "    def __init__(self, base_model_one, base_model_two, meta_model):\n",
    "        self.base_model_one = base_model_one\n",
    "        self.base_model_two = base_model_two\n",
    "        self.meta_model = meta_model\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "\n",
    "        self.base_model_one.fit(X_train, y_train)\n",
    "        self.base_model_two.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        pred_1 = self.base_model_one.predict_proba(X_val)[:, 1]\n",
    "        pred_2 = self.base_model_two.predict_proba(X_val)[:, 1]\n",
    "\n",
    "\n",
    "        blending_X = np.column_stack((pred_1, pred_2))\n",
    "        blending_y = y_val\n",
    "\n",
    "        self.meta_model.fit(blending_X, blending_y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        test_pred_1 = self.base_model_one.predict_proba(X_test)[:, 1]\n",
    "        test_pred_2 = self.base_model_two.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Create test features for the meta-learner\n",
    "        test_blending_X = np.column_stack((test_pred_1, test_pred_2))\n",
    "\n",
    "        # Use the meta-learner to make the final predictions\n",
    "        final_pred = self.meta_model.predict(test_blending_X)\n",
    "        return final_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1121d9-1c55-4b1d-8b4b-26ae9268f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res(pred, implied_prob):\n",
    "\n",
    "      if abs(pred - implied_prob) >= 0.1:\n",
    "            return implied_prob\n",
    "      return pred\n",
    "\n",
    "def try_features(feat_set, max_depth=2):\n",
    "    target = 'home_victory'\n",
    "    X_train = df_train.loc[:,feat_set]\n",
    "    X_valid = df_valid.loc[:,feat_set]\n",
    "    X_test = df_test.loc[:,feat_set]\n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy='mean')  # Or 'median', 'most_frequent', etc.\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_valid = imputer.transform(X_valid)\n",
    "    X_test = imputer.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    lgbm1 = lgbm.LGBMClassifier(n_estimators=1000, learning_rate=.02, max_depth=max_depth)\n",
    "    lgbm1.fit(X_train, y_train, eval_set=(X_valid, y_valid), eval_metric='logloss', \n",
    "          callbacks=[lgbm.early_stopping(stopping_rounds=50)])\n",
    "\n",
    "\n",
    "    preds_lgbm_test = lgbm1.predict_proba(X_test)[:,1]\n",
    "    ll_test = log_loss(y_test, preds_lgbm_test)\n",
    "    result = pd.DataFrame(preds_lgbm_test, columns=['result'])\n",
    "\n",
    "    dec_tree = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    log_reg = LogisticRegression(max_iter=1000)\n",
    "    random_forest = RandomForestClassifier(max_depth=max_depth)\n",
    "\n",
    "    stacking_clf = StackingClassifier(estimators=[\n",
    "                                                    # ('xgb', XGBClassifier(n_estimators=1000, learning_rate=.02, max_depth=max_depth)),\n",
    "                                                    # ('dec_tree', dec_tree),\n",
    "                                                    ('random_forest', random_forest),\n",
    "                                                    ('log_reg', log_reg)\n",
    "                                                    ],\n",
    "                                        final_estimator=LogisticRegression(max_iter=1000))\n",
    "\n",
    "\n",
    "    blending_model = Blending(lgbm1, stacking_clf, log_reg)\n",
    "\n",
    "    blending_model.fit(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "    # calibrated_stacking = CalibratedClassifierCV(blending_model, method='sigmoid', cv=\"prefit\")\n",
    "    # calibrated_stacking.fit(X_valid, y_valid)\n",
    "\n",
    "    stacking_preds = blending_model.predict(X_test)\n",
    "\n",
    "    new_model = pd.DataFrame(stacking_preds, columns=['result'])\n",
    "\n",
    "    implied_prob = pd.DataFrame()\n",
    "    implied_prob['implied_prob_h_mid'] = df_test['implied_prob_h_mid']\n",
    "\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['our_result'] = new_model['result']\n",
    "    new_df['implied_prob']  = implied_prob['implied_prob_h_mid']\n",
    "\n",
    "#     new_df['final_result'] = new_df.apply(lambda row: res(row.our_result, row.implied_prob), axis=1)\n",
    "\n",
    "    implied_prob['implied_prob_h_mid'] = (implied_prob['implied_prob_h_mid'] > .5).astype(int)\n",
    "    result['result'] = (result['result'] > 0.5).astype(int)\n",
    "    new_model['result'] = (new_model['result'] > 0.5).astype(int)\n",
    "\n",
    "    test_df = pd.DataFrame()\n",
    "    test_df['result'] = new_model['result']\n",
    "    test_df['implied_prob_h_mid'] = implied_prob['implied_prob_h_mid']\n",
    "\n",
    "#     our_accuracy_rate = round((accuracy_score(y_test, new_model['result']) * 100),2)\n",
    "    our_accuracy_rate = round((accuracy_score(y_test, new_df['our_result']) * 100),2)\n",
    "    yt_accuracy_rate = round((accuracy_score(y_test, result['result']) * 100), 2)\n",
    "\n",
    "#     difference_in_accuracy = round((accuracy_score(y_test, new_model['result']) * 100) - (accuracy_score(y_test, implied_prob['implied_prob_h_mid']) * 100), 2)\n",
    "    difference_in_accuracy = round((accuracy_score(y_test, new_df['our_result']) * 100) - (accuracy_score(y_test, implied_prob['implied_prob_h_mid']) * 100), 2)\n",
    "    difference_in_accuracy_yt = round((accuracy_score(y_test, result['result']) * 100) - (accuracy_score(y_test, implied_prob['implied_prob_h_mid']) * 100), 2)\n",
    "    vegas_accuracy_rate = round((accuracy_score(y_test, implied_prob['implied_prob_h_mid']) *100),2)\n",
    "\n",
    "    print(\"YT model's accuracy score: \", yt_accuracy_rate)\n",
    "    print(\"Vegas model's accuracy score: \", vegas_accuracy_rate)\n",
    "    print(\"Our model's accuracy score: \", our_accuracy_rate)\n",
    "    print(\"Difference in accuracy (YT): \", difference_in_accuracy_yt)\n",
    "    print(\"Difference in accuracy (Our):\", difference_in_accuracy)\n",
    "\n",
    "    return yt_accuracy_rate, vegas_accuracy_rate, our_accuracy_rate, difference_in_accuracy_yt, difference_in_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fcdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = [\n",
    "        \"Strt_WHIP_35_v\",\n",
    "        \"Strt_TB_BB_perc_35_h\",\n",
    "        \"Strt_TB_BB_perc_35_v\",\n",
    "        \"Strt_H_BB_perc_35_h\",\n",
    "        \"Strt_H_BB_perc_35_v\",\n",
    "        \"Strt_SO_perc_10_h\",\n",
    "        \"Strt_SO_perc_10_v\",\n",
    "        \"Bpen_WHIP_75_h\",\n",
    "        \"Bpen_WHIP_75_v\",\n",
    "        \"Bpen_TB_BB_perc_75_h\",\n",
    "        \"Bpen_TB_BB_perc_75_v\",\n",
    "        \"Bpen_H_BB_perc_75_h\",\n",
    "        \"Bpen_H_BB_perc_75_v\",\n",
    "        \"Bpen_SO_perc_75_h\",\n",
    "        \"Bpen_SO_perc_75_v\",\n",
    "        \"Bpen_WHIP_35_h\",\n",
    "        \"Bpen_WHIP_35_v\",\n",
    "        \"Bpen_TB_BB_perc_35_h\",\n",
    "        \"Bpen_TB_BB_perc_35_v\",\n",
    "        \"Bpen_H_BB_perc_35_h\",\n",
    "        \"Bpen_H_BB_perc_35_v\",\n",
    "        \"prev_year_home_win_rate\",\n",
    "        \"avg_runs_allowed_h\",\n",
    "        \"avg_runs_allowed_v\",\n",
    "        \"avg_runs_h\",\n",
    "        \"avg_runs_v\",\n",
    "        \"prev_year_visitor_win_rate\",\n",
    "        \"days_between_games_h\",\n",
    "        # \"stadium_ANA\",\n",
    "        # \"stadium_ARI\",\n",
    "        # \"stadium_ATL\",\n",
    "        # \"stadium_BAL\",\n",
    "        # \"stadium_BOS\",\n",
    "        # \"stadium_CAL\",\n",
    "        # \"stadium_CHA\",\n",
    "        # \"stadium_CHN\",\n",
    "        # \"stadium_CIN\",\n",
    "        # \"stadium_CLE\",\n",
    "        # \"stadium_COL\",\n",
    "        # \"stadium_DET\",\n",
    "        # \"stadium_FLO\",\n",
    "        # \"stadium_HOU\",\n",
    "        # \"stadium_KCA\",\n",
    "        # \"stadium_LAN\",\n",
    "        # \"stadium_MIA\",\n",
    "        # \"stadium_MIL\",\n",
    "        # \"stadium_MIN\",\n",
    "        # \"stadium_MON\",\n",
    "        # \"stadium_NYA\",\n",
    "        # \"stadium_NYN\",\n",
    "        # \"stadium_OAK\",\n",
    "        # \"stadium_PHI\",\n",
    "        # \"stadium_PIT\",\n",
    "        # \"stadium_SDN\",\n",
    "        # \"stadium_SEA\",\n",
    "        # \"stadium_SFN\",\n",
    "        # \"stadium_SLN\",\n",
    "        # \"stadium_TBA\",\n",
    "        # \"stadium_TEX\",\n",
    "        # \"stadium_TOR\",\n",
    "        # \"stadium_WAS\",\n",
    "        \"days_between_games_v\",\n",
    "        \"hour_diff\",\n",
    "        \"implied_prob_h\",\n",
    "        \"implied_prob_v\",\n",
    "        \"implied_prob_h_mid\",\n",
    "        \"Bpen_SO_perc_35_h\",\n",
    "        \"Bpen_SO_perc_35_v\",\n",
    "        \"Bpen_WHIP_10_h\",\n",
    "        \"Bpen_WHIP_10_v\",\n",
    "        \"Bpen_TB_BB_perc_10_h\",\n",
    "        \"Bpen_TB_BB_perc_10_v\",\n",
    "        \"Bpen_H_BB_perc_10_h\",\n",
    "        \"Bpen_H_BB_perc_10_v\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d553c-5777-45f8-b494-89e42770e597",
   "metadata": {
    "tags": []
   },
   "source": [
    "### First, let's revisit our best model from our last modeling session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_accuracy_rate, vegas_accuracy_rate, our_accuracy_rate, difference_in_accuracy_yt, difference_in_accuracy = try_features(best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2bf442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.run_diff!=0]\n",
    "df = df[best_features + ['home_victory', 'season']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed916b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df null value count: \", df[best_features].isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7466067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(best_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')  # Or 'median', 'most_frequent', etc.\n",
    "df[best_features] = imputer.fit_transform(df[best_features])  # Ensure that you're only imputing relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df[best_features].describe() - df_before[best_features].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before[best_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72bea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa747810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Apply KMeans clustering to group similar points\n",
    "kmeans = KMeans(n_clusters=9, random_state=42, n_init=10, max_iter=5000)\n",
    "\n",
    "kmeans.fit(df[best_features])  # Fit the model\n",
    "\n",
    "df['bin'] = kmeans.predict(df[best_features])  # Assign bins to the data\n",
    "\n",
    "df_train = df[(df.season>1980) & (df.season<=2016)]\n",
    "df_valid = df[(df.season>=2017) & (df.season<=2018)]\n",
    "df_test = df[df.season>=2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "clusters = dbscan.fit_predict(df[best_features])\n",
    "df['cluster'] = clusters\n",
    "\n",
    "df_train = df[(df.season>1980) & (df.season<=2016)]\n",
    "df_valid = df[(df.season>=2017) & (df.season<=2018)]\n",
    "df_test = df[df.season>=2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Feature data for clustering (replace X with your actual data)\n",
    "X = df_train[best_features]\n",
    "\n",
    "# Try different numbers of clusters\n",
    "inertia = []\n",
    "cluster_range = range(2, 15)  # Test from 2 to 14 clusters\n",
    "\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(cluster_range, inertia, marker='o')\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Inertia (Sum of Squared Distances)\")\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge bins with fewer samples\n",
    "\n",
    "# df_train['bin'] = df_train['bin'].replace({2: 3})\n",
    "print(\"Training set distribution:\")\n",
    "print(df_train['cluster'].nunique())\n",
    "print(\"Validation set distribution:\")\n",
    "print(df_valid['cluster'].nunique())\n",
    "print(\"Test set distribution:\")\n",
    "print(df_test['cluster'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6795f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate column names\n",
    "duplicate_columns = df_train.columns[df_train.columns.duplicated()]\n",
    "print(f\"Duplicate columns: {duplicate_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b989b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bin_one = df_train[df_train['bin'] == 1]\n",
    "val_bin_one = df_valid[df_valid['bin'] == 1]\n",
    "test_bin_one = df_test[df_test['bin'] == 1]\n",
    "\n",
    "print(\"Train Bin 1 Mean Values:\\n\", train_bin_one.mean(numeric_only=True))\n",
    "print(\"Validation Bin 1 Mean Values:\\n\", val_bin_one.mean(numeric_only=True))\n",
    "print(\"Test Bin 1 Mean Values:\\n\", test_bin_one.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5007aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def get_model(model_name):\n",
    "\n",
    "    if model_name == 'log_reg':\n",
    "\n",
    "        return LogisticRegression(max_iter=1000)\n",
    "\n",
    "    elif model_name == 'dec_tree':\n",
    "\n",
    "        return DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "    elif model_name == 'random_forest':\n",
    "\n",
    "        return RandomForestClassifier(max_depth=2)\n",
    "\n",
    "    elif model_name == 'xgb':\n",
    "\n",
    "        return XGBClassifier(n_estimators=1000, learning_rate=.02, max_depth=2)\n",
    "\n",
    "    elif model_name == 'lgbm':\n",
    "\n",
    "        return lgbm.LGBMClassifier(n_estimators=1000, learning_rate=.02, max_depth=2)\n",
    "\n",
    "    elif model_name == 'SVC':\n",
    "\n",
    "        return SVC(probability=True)\n",
    "\n",
    "    elif model_name == 'neural_net':\n",
    "\n",
    "        return MLPClassifier(hidden_layer_sizes=(64, 32),  # Two hidden layers (64 and 32 neurons)\n",
    "                               activation='relu',  # Use ReLU activation\n",
    "                               solver='adam',  # Adam optimizer\n",
    "                               max_iter=500,  # Train for 500 epochs\n",
    "                               random_state=42)\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(model_name)\n",
    "        print(\"Model not found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "models = ['log_reg', 'dec_tree', 'random_forest', 'xgb', 'lgbm', 'SVC']\n",
    "\n",
    "model_combinations = []\n",
    "\n",
    "model_combinations = [list(comb) for i in range(1, len(models) + 1) for comb in combinations(models, i)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c89b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for combo in model_combinations:\n",
    "    print(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test['cluster'].nunique() * len(model_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0eb4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "right_preds = 0\n",
    "total_preds = 0\n",
    "\n",
    "max_depth = 2\n",
    "\n",
    "best_models = {\n",
    "\n",
    "}\n",
    "\n",
    "for bin_label in df_train['cluster'].unique():\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_estimators = None\n",
    "\n",
    "    for combo in model_combinations:\n",
    "        estimators = []\n",
    "\n",
    "        bin_df = df_train[df_train['cluster'] == bin_label]\n",
    "        X_train_bin = bin_df[best_features]\n",
    "        y_train_bin = bin_df[target]\n",
    "\n",
    "        X_val_bin = df_valid[df_valid['cluster'] == bin_label][best_features]\n",
    "        y_val_bin = df_valid[df_valid['cluster'] == bin_label][target]\n",
    "\n",
    "        X_test_bin = df_test[df_test['cluster'] == bin_label][best_features]\n",
    "        y_test_bin = df_test[df_test['cluster'] == bin_label][target]\n",
    "\n",
    "        if X_test_bin.empty:\n",
    "            continue\n",
    "\n",
    "        estimators.append((model, get_model(model)) for model in combo)\n",
    "\n",
    "        stacking_clf = StackingClassifier(estimators=estimators,\n",
    "                                        final_estimator=get_model('log_reg'), cv=5)\n",
    "\n",
    "        stacking_clf.fit(X_train_bin, y_train_bin)\n",
    "\n",
    "        blending_preds = stacking_clf.predict(X_test_bin)\n",
    "\n",
    "        if accuracy_score(y_test_bin, blending_preds) > best_accuracy:\n",
    "            best_accuracy = accuracy_score(y_test_bin, blending_preds)\n",
    "            best_estimators = copy.deepcopy(estimators)\n",
    "\n",
    "    right_preds += best_accuracy * len(y_test_bin)\n",
    "    best_models[bin_label] = copy.deepcopy(best_estimators)\n",
    "    total_preds += len(y_test_bin)\n",
    "\n",
    "print(\"Accuracy Rate: \", round((right_preds / total_preds) * 100, 2))\n",
    "\n",
    "for label in best_models:\n",
    "    print(\"Cluster: \", label)\n",
    "    print(\"Best Models: \", best_models[label])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5fcefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "right_preds = 0\n",
    "total_preds = 0\n",
    "\n",
    "max_depth = 2\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "# Assuming best_features contains both numerical and categorical features\n",
    "numerical_features = [feature for feature in best_features if df_train[feature].dtype in ['int64', 'float64']]\n",
    "categorical_features = [feature for feature in best_features if df_train[feature].dtype == 'object']\n",
    "\n",
    "# Define the transformations for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine the transformations into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "for bin_label in df_train['cluster'].unique():\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_estimators = None\n",
    "\n",
    "    for combo in model_combinations:\n",
    "        bin_df = df_train[df_train['cluster'] == bin_label]\n",
    "        X_train_bin = bin_df[best_features]\n",
    "        y_train_bin = bin_df[target]\n",
    "\n",
    "        X_val_bin = df_valid[df_valid['cluster'] == bin_label][best_features]\n",
    "        y_val_bin = df_valid[df_valid['cluster'] == bin_label][target]\n",
    "\n",
    "        X_test_bin = df_test[df_test['cluster'] == bin_label][best_features]\n",
    "        y_test_bin = df_test[df_test['cluster'] == bin_label][target]\n",
    "\n",
    "        if X_test_bin.empty:\n",
    "            continue\n",
    "\n",
    "        # Apply the preprocessor to the training, validation, and test sets\n",
    "        X_train_bin = preprocessor.fit_transform(X_train_bin)\n",
    "        X_val_bin = preprocessor.transform(X_val_bin)\n",
    "        X_test_bin = preprocessor.transform(X_test_bin)\n",
    "\n",
    "        estimators = [(model, get_model(model)) for model in combo]\n",
    "\n",
    "        stacking_clf = StackingClassifier(estimators=estimators,\n",
    "                                          final_estimator=get_model('neural_net'), cv=5)\n",
    "\n",
    "        stacking_clf.fit(X_train_bin, y_train_bin)\n",
    "\n",
    "        blending_preds = stacking_clf.predict(X_test_bin)\n",
    "\n",
    "        if accuracy_score(y_test_bin, blending_preds) > best_accuracy:\n",
    "\n",
    "            best_accuracy = accuracy_score(y_test_bin, blending_preds)\n",
    "            best_estimators = copy.deepcopy(estimators)\n",
    "\n",
    "    right_preds += best_accuracy * len(y_test_bin)\n",
    "    best_models[bin_label] = copy.deepcopy(best_estimators)\n",
    "    total_preds += len(y_test_bin)\n",
    "\n",
    "print(\"Accuracy Rate: \", round((right_preds / total_preds) * 100, 2))\n",
    "\n",
    "for label in best_models:\n",
    "\n",
    "    print(\"Cluster: \", label)\n",
    "    print(\"Best Models: \", best_models[label])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
